# ğŸ§  Toxic Comment Detector with BERT + Streamlit

Este proyecto es una aplicaciÃ³n interactiva que detecta comentarios tÃ³xicos utilizando un modelo BERT fine-tuneado para clasificaciÃ³n multilabel.

## ğŸ“Œ DescripciÃ³n
El modelo analiza textos en **idioma inglÃ©s** y clasifica los comentarios segÃºn seis categorÃ­as de toxicidad:

- `toxic`
- `severe_toxic`
- `obscene`
- `threat`
- `insult`
- `identity_hate`

## ğŸ§  TecnologÃ­as utilizadas

- HuggingFace Transformers
- PyTorch
- Streamlit
- Scikit-learn

## ğŸš€ CÃ³mo ejecutar la aplicaciÃ³n

### 1. Clonar el repositorio
```bash
git clone https://github.com/tu-usuario/toxic-comment-detector.git
cd toxic-comment-detector
```

### 2. Instalar dependencias
```bash
pip install -r frontend/requirements.txt
```

### 3. Ejecutar la app
```bash
cd frontend
streamlit run app_hf.py
```

âš ï¸ AsegÃºrate de tener la carpeta `modelo_bert_final/` dentro del mismo directorio de `app.py`.

## ğŸ“Š Resultados del modelo (F1-score promedio)

| Modelo         | F1 promedio |
|----------------|-------------|
| Logistic       | 0.194       |
| Random Forest  | 0.260       |
| LinearSVC      | 0.370       |
| Red Neuronal   | 0.454       |
| **BERT (final)**| **0.604**  âœ…

## ğŸ“ Estructura del proyecto

```
toxic-comment-detector/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ model_training.ipynb
â”‚   â””â”€â”€ modelo_bert_final/
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â””â”€â”€ train.csv
â”œâ”€â”€ results/
â”‚   â””â”€â”€ tabla_comparativa_final.csv
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â””â”€â”€ .gitignore
```

## ğŸ§‘â€ğŸ’» Autor
Felipe Villa Â· Desarrollador de modelos NLP y apps interactivas de IA.
